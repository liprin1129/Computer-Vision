{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Fourier Transform Sampling Pairs](#Fourier Transform Sampling Pairs)\n",
    "2. [Sampling and reconstruction](#Sampling and reconstruction)\n",
    "    1. [Reconstruction](#Reconstruction)\n",
    "    2. [Sampling](#Sampling)\n",
    "    3. [Undersampling](#Undersampling)\n",
    "    4. [Aliasing](#Aliasing)\n",
    "    5. [Antialiasing](#Antialiasing)\n",
    "5. [Sampleing Frequency Signal](#Sampleing Frequency Signal)\n",
    "    1. [Impuls Train](#Impuls Train)\n",
    "    2. [Sampling Low Frequency Signal](#Sampling Low Frequency Signal)\n",
    "    3. [Sampling High Frequency Signal](#Sampling High Frequency Signal)\n",
    "    4. [Aliasing in Images](#Aliasing in Images)\n",
    "6. [Campbell-Robson Contrast Sensitivity](#Campbell-Robson Contrast Sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Fourier Transform Sampling Pairs <a name='Fourier Transform Sampling Pairs'></a>\n",
    "\n",
    "#### 1) Notion of a pulse train (or a set of impluses)\n",
    "\n",
    "The Fourier transform of pusle train is another impulse train. \n",
    "\n",
    "The thing that we need to realise is __the scaling theorem__. As the puses in space get further apart, the pulses in frequency get closer together. For example, if the pulse in time domain is in the middle and the next ones are infinitely far far away, that would be just like having sort of a sigle impulse, then the Fourier transform of the single impulse will be 1, because the frequency has to keep all the frequencies. \n",
    "\n",
    "<img src=\"images/9/1.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Sampling and reconstruction <a name='Sampling and reconstruction'></a>\n",
    "\n",
    "### 2.1. Reconstruction <a name='Reconstruction'></a>\n",
    "<img src=\"images/9/2.png\" width=\"400px\">\n",
    "Nice smooth scene illuminated by a light source and then imaging system captures the intensity at a bunch of discrete places.\n",
    "<img src=\"images/9/3.png\" width=\"400px\">\n",
    "We would like to take the captures from these discrete places, and these discrete places can be reconstructed to the smooth signal.\n",
    "\n",
    "Let's talk a little bit about sampling.\n",
    "\n",
    "### 2.2. Sampling <a name='Sampling'></a>\n",
    "\n",
    "How do we store a continuous signal in a computer?\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 20.14.10.png' width=500>\n",
    "\n",
    "Above is a one dimentional array, and we take the samples at each different location. Once we have that set of samples, later we want to reconstruct what the original signal. The reason we can know easily for an audio and a speaker, or mathematical processing in terms of what this signal is. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 21.39.00.png' width=500>\n",
    "\n",
    "Actually, what we have to do is \"guessing\"; what the function did in between (__reconstruction__).\n",
    "\n",
    "For example: recording - sound to analog to samples to disc, playback - disc to samples to analog to sound again.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 21.48.26.png' width=500>\n",
    "\n",
    "If I collect a bunch of samples, then what am I going to do to reconstruct it?\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 21.55.10.png' width=500>\n",
    "\n",
    "It might be enough to basically just to connect the dots and get back the next sine wave.\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 21.56.26.png' width=500>\n",
    "\n",
    "So, as long as I had enough dots, that's easy enough to do. \n",
    "\n",
    "### 2.3. Undersampling <a name='Undersampling'></a>\n",
    "\n",
    "If we missed things going on between the samples, what if we don't have enough dots?.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 21.59.59.png' width=500>\n",
    "\n",
    "Some information is getting lost (all changes going on in is not realized or not captured). As a result, the original high frequency is indistinguishable from lower frequency.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 22.21.28.png' width=500>\n",
    "\n",
    "Also, the original low frequency is always indistinguishable from those higher frequencies if only gave a small number of dots. That is we can't tell whether or not it's a high frequency or a low frequency, and that is the definition of aliasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.4. Aliasing <a name='Aliasing'></a>\n",
    "\n",
    "it travels sort of in disguise of another signal or of other frequencies. It is \"an effect that causes different signals to become indistinguishable (or aliases of one another) when sampled. It also refers to the distortion or artifact that results when the signal reconstructed from samples is different from the original continuous signal\" ([wikipedia](https://en.wikipedia.org/wiki/Aliasing)).\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 22.38.39.png' width=500>\n",
    "\n",
    "Essentially, the thing is moving too fast for how often we're sampling a time for us to actually be able to tell what's going on. The high frequency and the low frequency can't be distinguished.\n",
    "\n",
    "For example:\n",
    "\n",
    "Here we have a rendering.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 22.49.32.png' width=300>\n",
    "\n",
    "As this checkerboard gets further away in the distance, it's supposed to start getting thinner and thinner and closer and closer. However, somewhere right around, it starts to break up. It is like low frequency all over again. \n",
    "\n",
    "If we plot the left input signal as an image in MATLAB, then it would look like the right.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 22.53.01.png' width=500>\n",
    "\n",
    "That is ailasing. There are not enough pixels in the plot for us to be able to see what's going on. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-22 at 22.57.08.png' width=400>\n",
    "\n",
    "We have a small number of samples, a not dense enough sampling, in order to recover those frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5. Antialiasing <a name='Antialiasing'><a/>\n",
    "\n",
    "How can we prevent aliasing from happening? \n",
    "\n",
    "1. **Sample more often**: \n",
    "    - Join the Mega-Pixel craze of the photo industry. \n",
    "    - But this can't go on forever.\n",
    "2. __Make the signal less \"wiggly\"__:\n",
    "    - Get rid of some high frequencies.\n",
    "    - Will lose information.\n",
    "    - But it's better than aliasing.\n",
    "    \n",
    "For example, we can introduce lowpass filters to audio input just after microphone, so as to remove high frequencies leaving only safe, low frequencies to be reconstructed. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.16.46.png' width=500>\n",
    "\n",
    "So, that syas that we can reduce the number of samples we need to take, or limit the number of samples we need to take, and then when we do the reconstruction, we'll know that anything that was reconstructed of a higher frequency than we let in should be thrown away. So, we use the lopass filter again for the output to the speacker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sampleing Frequency Signal <a name='Sampleing Frequency Signal'></a>\n",
    "\n",
    "### 5.1. Impuls Train. <a name='Impuls Train'></a>\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.30.33.png' width=500>\n",
    "\n",
    "Delta function, $\\delta$, is a function that's a pulse of one when its argument is zero. What this would mean here is as $k$ goes from minus infinity to infinity, $[x-kM]$ would count up by $M$s, so $x$ every $M$ would be a one. So, if $M$ gets bigger, the pulse get further and further apart. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.33.03.png' width=500>\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.33.43.png' width=300>\n",
    "\n",
    "The Fourier transform of an impulse train is an another impulse train, and the spacing gets larger in space, it gets narrower in frequency (that was the scaling property of the Fourier transform). So the less often we sample in space, the higher the samples in frequency. \n",
    "\n",
    "Impulse in 2D is called __bed of nails__, because instead of just having a train of pulses that land on the integers or some multiple of integers in 1D, it's actually lying out on all the discrete coordinates in 2D.\n",
    "\n",
    "$comb$ of $M, N$ because we have a separation both in the $x$ direction and in the $y$ direction.\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.38.48.png' width=400>\n",
    "\n",
    "Also in the same way that the Fourier transform of an impulse train is an impulse train, the Fourier transform of a bed of nails is another bed of nails. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.40.24.png' width=400>\n",
    "\n",
    "As the nails get further apart, the Fourier ones get closer together.\n",
    "\n",
    "We're going to now talk about aliasing in the Fourier space, and we're going to talk about sampling a signal. And we'll first talk about sampling a low frequency signal, and then a high frequency signal.\n",
    "\n",
    "### 5.2. Sampling Low Frequency Signal <a name='Sampling Low Frequency Signal'></a>\n",
    "\n",
    "Sampling is just multiplying the continuous signal by the discrete comb. \n",
    "\n",
    "Now, when I multiply in space, what do we do in frequency? \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.51.05.png' width=500>\n",
    "\n",
    "I convolve, because onvolution in space is multiply in frequency, and multiply in space is convolution in frequency. \n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.53.10.png'\n",
    "width=480>\n",
    "\n",
    "If the Fourier transform function is limited then if I could pull out that part of the signal, the part of the signal that's frequency is just within some range. Then, we would be able to get back the original spectrum, which would give back the original function. \n",
    "\n",
    "So this is no problem if the outer edge in Frourier transform is small enough, that the maximum frequency of the signal is low enough for the $comb$ filter that we used. Exactly, what is that small enough?\n",
    "\n",
    "<img src='Images/9/Screen Shot 2017-03-23 at 00.59.00.png' width=500>\n",
    "\n",
    "If my $comb$ filter is spacing $M$, then the $comb$ of the Fourier of the comb is 1 over $M$, where $W$ is bandwidth. Half this distance here is 1 over 2M. If the maximum frequency of that function, let's say is W, for bandwidth, less than 1 over 2M, then I can reconstruct that original signal by looking at just the part, that is, nothing has contaminated that inner part. So, if there's no overlap, if W is less than 1 over 2M, the original signal can be recovered from its samples by low-pass filtering. \n",
    "\n",
    "For example of Nyquist sampling, if I want to recover, say, something up to 20 kilohertz, right, 20 kilohertz, about the extent of human hearing, I would need to sample at least 40 kilohertz. By the way, CDs, were talking about, sample at 44 kilohertz, why? So that we can recover everything up to 22 kilohertz, which is the extent of human hearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Sampling High Frequency Signal <a name='Sampling High Frequency Signal'></a>\n",
    "\n",
    "So, as long as the frequencies are low enough, we're okay. But what if they're not low enough, what happens? \n",
    "\n",
    "Let's suppose some random function, f of x, containing som higher frequency. Then, bandwidth, w, of f of u goes out a little bit further.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.24.56.png' width = 400>\n",
    "\n",
    "If now, we multiply that by the same comb of M, we're going to convolve that in Furiour space with that 1 over M comb. To be noted that the region where frequencies folded with actually sum (red region on below figure). So, these things would sum and come back up there.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.28.16.png' width = 400>\n",
    "\n",
    "And that area is where essentially the energy has folded back on itself, it's overlapped and then they're going to add.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.31.15.png' width = 500>\n",
    "\n",
    "The problem occur at where high frequency that was really on little edge of low frequency, or it's the thing that is pretending it's low frequency. \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.35.46.png' width = 100>\n",
    "\n",
    "That's the aliasing. Once we've aliased the signal, once we've done this operation, we cannot undo it. The high frequencies have pretended that they're low frequencies, and we can't separate them out from real low frequencies. We have to remove the high frequencies before we do the sampling, in order to remove the aliasing.\n",
    "\n",
    "In order remove high frequencies, we convolve with h, which maybe its another Gaussian.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.41.41.png' width = 500>\n",
    "\n",
    "That reduces the high frequencies, so the frequency spectrum of the convolution of f and h. h is what's referred to as an anti-aliasing filter, and it has the net effect of trimming the high frequencies of the original signal. \n",
    "\n",
    "Then, when we do the multiplication by the comb, which is shown below,\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.48.35.png' width = 500>\n",
    "\n",
    "we are now convolving that with a comb filter in the frequency domain. So no aliasing goes on. And then to recover the signal, whatever we get after we reconstruct from that using the new sampled, convolved function, we can throw away any high frequencies because those are not real.\n",
    "\n",
    "As just the way we did with the CD, low pass filtered the microphone did a A to D conversion to get some digital samples, we did a D to A conversion to get an analog. And we knew that any high frequency that showed up, that's not real.\n",
    "\n",
    "To sum up in terms of aliasing, \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 18.50.19.png' width = 500>\n",
    "\n",
    "when we don't have an anti-aliasing filter, we get overlap regions that cause to get aliasings that cannot be removed once we've done it. But, with an anti-aliasing filter, we don't get that overlap, so, that's the mathematics behind aliasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Aliasing in Images <a name='Aliasing in Images'></a>\n",
    "\n",
    "This is an example showing you aliasing in images of a radial sine wave.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.04.22.png' width=400>\n",
    "\n",
    "If we can see as getting closer and closer, the pixels are supposed to be wiggling faster and faster. Eventually, it runs out of pixels. We don't have enough samples to get how quickly the thing varies, and so that is an example of aliasing here. \n",
    "\n",
    "So, what is its impact on? Suppose we've got the image, a Van Gogh picture. \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.10.09.png' width=400>\n",
    "\n",
    "Suppose it's too big to fit on a screen, so we want a smaller version of it. One obvious way to make it smaller would be to throw away every other column and every other row. And that's called image sub-sampling. What it means is we throw away half the columns and half the rows, which of course is actually a quarter size smaller, but half meaning, half the rows, half the columns. If we did that again, we would now have one quarter of the total rows, one quarter of the total columns, one-sixteenth the size. We could do it again, one-eighth.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.12.43.png' width=530>\n",
    "\n",
    "In fact, we might start to notice one-eighth version is already looking a little ugly when we blow up the half, the quarter and the eighth by zooming in on both of them just blowing up the pixels. \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.17.21.png' width=500>\n",
    "\n",
    "The one-half one doesn't look too bad, but remember we've only taken one out of every eight columns and one out of every eight rows, and then we blew the thing back up. However, these does not look like a blurry version of the original, because the reason is we have aliasing. We have sampled too infrequently for the amount of variation that there is. Essentially, things wiggling here more often than one out of every eight. So, if we just take one eighth, we're not capturing all the wiggles. \n",
    "\n",
    "We need to do that anti-aliasing. A simple one is to do Gaussian. We'll filter, so we'll take the Gaussian and then take every other row to get a half. Then we could do a Gaussian of that, and take every other row of that, to get the one quarter. And we could do the Gaussian again, and do that. Or, what we could do is we do a much bigger Gaussian to begin with, and take one out of every eight. We can convince ourself that mathematically these are going to be equivalent.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.24.00.png' width=500>\n",
    "\n",
    "When we do that, and blow up the Gaussian one-eighth, we'll see that this actually looks pretty much like a blurry version of that.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-29 at 19.25.57.png' width=500>\n",
    "\n",
    "We can see that the Gaussian does a much better job doing the reduction. That's because it's been anti-aliased. So if we're doing Computer Vision and we decide what we've got a huge 1920 by 1080 image coming in, but we're just trying to recognize faces, 100 pixels.  \n",
    "\n",
    "It's expensive to do that over the entire image, when it's so big. So, Let's make the image one-eighth its size that way. And, we'll make my faces smaller, and look for them that way. That will work pretty well, if we do anti-aliasing. If we just pull out the arbitrary pixels, we're going to make a mess of the whole thing. We'll get aliasing, and then the faces won't look right. So that's why doing this kind of stuff matters even in computer vision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Campbell-Robson Contrast Sensitivity <a name='Campbell-Robson Contrast Sensitivity'></a>\n",
    "\n",
    "Frequency is so prevalent in the processing that we're going to be doing. Some psycho physicists, Campbell and Robson studied a long time ago the contrast sensitivity of the human vision system. Now what we're looking at below is a curve where frequency goes up to the right and contrast goes up down. And probably, if we're like most humans, we probably can see this stuff really well, but not so much up there.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 00.33.25.png' width=400>\n",
    "\n",
    "And somewhere around left and right up side, that curve is where the fall off is. That essentially, areas where the lower contrast (left up side), we can't really see the variations. Same thing with this high contrast (right up side). Our eye is sensitive in different ways. \n",
    "\n",
    "So what this means is that some frequencies matter more in terms of the importance to us in an image. In particular, if we were to leave out a bunch of high frequencies, so that is, if there was low contrast at the high frequency, we wouldn't even know, so we can use that to do image compression.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 00.32.18.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Image Compression <a name='Image Compression'></a>\n",
    "\n",
    "We probably have heard of JPEG. JPEG uses something called the _Discrete Cosine Transform_, or variations there of. Basically a way of thinking about that is they take a little 8x8 region  of the image. So we carve the whole region up in little 8x8 segments.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 00.41.12.png' width=500>\n",
    "\n",
    "And then it takes, as a basis set, sinusoids and co-sinusoids. So at the right side of figure, we have the ones that are vertical (row 0), there we have ones that are coming down (column 0) and there we have the product of them (other elements). \n",
    "\n",
    "And we can see the lower frequencies are in the top left hand corner and the higher frequencies are in the bottom right hand corner. So, what we can do is how much of each of these do we need to make the picture?\n",
    "\n",
    "We can actually order them, right. We can sort of go through them and order them the away of arrows.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 00.45.53.png' width=500>\n",
    "\n",
    "With the idea being that the top left hand corner might be a constant that's just the average of the picture. So the top left hand corner, what sometimes called B(0,0) that's just the DC component, the average and then as we get further and further we getting the higher and higher frequencies. So the top left hand corner represent the lower frequencies. The bottom right hand corner represents the higher frequencies.\n",
    "\n",
    "Now, what we said before is, we don't have to represent the higher frequencies that well. Just sort of the high contrasty components of it. So one way of doing that is saying, what if we encode B(0,0) coefficients better, more bits, than B(7,7) coefficients? \n",
    "\n",
    "That gives what's called a quantization table.\n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 00.50.53.png' width=500> \n",
    "\n",
    "DCT, discrete cosine transform, does the compression by concentrating most image information in the low frequency. What this 3 means at B(0,0) is we keep the coefficient to the nearest three values, so if we're thinking of this in bits, we can shift it over by two. The idea is that we round this to the nearest 3. Whereas B(7,7) is rounded to the nearest 31. In other words, we're doing more representation of the top left hand corner than we are of the bottom right hand corner. \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 13.59.03.png' width=500>\n",
    "\n",
    "The same way, by the way, that we do the inverse Fourier transform given the spectrum you can reconstruct the signal. The same is true of the DCT. IDCT, inverse DCT. What that means is we lose information, but mostly lose the information of the high frequencies.\n",
    "\n",
    "Given those coefficients, We can reconstruct the image. But now, instead of maintaining all those coefficients exactly, we are only going to keep more information about the low frequency coefficients than about the high frequency coefficients.\n",
    "\n",
    "That works really well for the human vision system. \n",
    "\n",
    "<img src='images/9/Screen Shot 2017-03-30 at 14.02.34.png' width=500>\n",
    "\n",
    "They used 89,000 bytes of information to represent this picture using just the raw intensities and 12,000 bytes, so a ratio is seven to one, using DCT coefficients, the JPEG standard. The reason it works, to sum this all up, is that images vary over frequency, that's our base's set. \n",
    "\n",
    "The human vision system is sensitive to different frequencies at different amounts. By doing similar to a Fourier transform where you take that integral or that sum, that dot product with the wiggly sinusoids, DCT is a variant of that. We can figure out the coefficients for each of the frequencies. Then we can threshold essentially or reduce the level of representation of the high frequency in order to save a bunch of bits in describing the image."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
